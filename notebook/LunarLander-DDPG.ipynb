{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LunarLanderContinuous-v2\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (8,), float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, length):\n",
    "        self.memory = deque(maxlen=length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, size):\n",
    "        sample = random.sample(self.memory, size)\n",
    "\n",
    "        state = [i[0] for i in sample]\n",
    "        action = [i[1] for i in sample]\n",
    "        reward = [i[2] for i in sample]\n",
    "        next_state = [i[3] for i in sample]\n",
    "        terminal = [i[4] for i in sample]\n",
    "\n",
    "        state = np.stack(state)\n",
    "        state = torch.Tensor(state).squeeze()\n",
    "\n",
    "        next_state = np.stack(next_state)\n",
    "        next_state = torch.Tensor(next_state).squeeze()\n",
    "\n",
    "        reward = np.array(reward)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "        terminal = np.array(terminal).astype(int)\n",
    "        terminal = torch.tensor(terminal).reshape(-1, 1)\n",
    "\n",
    "        action = np.array(action)\n",
    "        action = torch.tensor(action, dtype=torch.float32)\n",
    "\n",
    "        return state, action, reward, next_state, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=[64, 64]):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "\n",
    "        input_dims = [state_dim] + hidden_dim\n",
    "        output_dims = hidden_dim + [action_dim]\n",
    "\n",
    "        for in_dim, out_dim in zip(input_dims, output_dims):\n",
    "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
    "\n",
    "        for i in range(len(hidden_dim)):\n",
    "            self.activations.append(nn.LeakyReLU())\n",
    "\n",
    "        self.activations.append(nn.Tanh())\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for l, activation in zip(self.layers, self.activations):\n",
    "            x = l(x)\n",
    "            x = activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, output_dim=1):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim + action_dim, hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.l1(state))\n",
    "        x = F.relu(self.l2(torch.cat([x, action], dim=-1)))\n",
    "        x = self.l3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=1000):\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = max_sigma\n",
    "        self.max_sigma = max_sigma\n",
    "        self.min_sigma = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim = action_space\n",
    "        self.reset()\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.00001\n",
    "        self.epsilon_min = 0.05\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def evolve_state(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "    def get_action(self, action, t=0):\n",
    "        ou_state = self.evolve_state() * self.epsilon\n",
    "\n",
    "        self.epsilon -= self.epsilon_decay\n",
    "        if self.epsilon < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_min, action_max, gamma=0.99):\n",
    "        super(DDPGAgent, self).__init__()\n",
    "        self.action_min = np.array(action_min)\n",
    "        self.action_max = np.array(action_max)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.ou_noise = OUNoise(action_dim)\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "\n",
    "        self.actor_target = Actor(state_dim, action_dim)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "        \n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        ################################################################\n",
    "        # Task 1: Create a replay memory with maximum capacity=1000\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        ################################################################\n",
    "        self.batch_size = 50\n",
    "        self.tau = 0.005\n",
    "        \n",
    "        self.num_fit = 0\n",
    "\n",
    "        self.loss_ftn = nn.MSELoss()\n",
    "\n",
    "    def get_action(self, state, t=0):\n",
    "        action_before_norm = self.actor(state).detach().numpy() # ranges in [-1, 1]\n",
    "        action_before_norm = self.ou_noise.get_action(action_before_norm, t)# ranges in [-1, 1]\n",
    "        \n",
    "        action = np.clip(action_before_norm, self.action_min, self.action_max)\n",
    "        return action  #, action\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.push(transition)\n",
    "    \n",
    "    def update_target(self, source, target, tau):\n",
    "        for src_param, target_param in zip(source.parameters(), target.parameters()):\n",
    "            target_param.data.copy_(tau * src_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def fit(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0, 0\n",
    "            \n",
    "        state, action, reward, next_state, terminal = self.memory.sample(self.batch_size)\n",
    "\n",
    "        q = self.critic(state, action)\n",
    "        \n",
    "        next_q_val = self.critic_target(next_state, self.actor_target(next_state))\n",
    "        ################################################################\n",
    "        # Task 3: Complete the critic loss calculation\n",
    "        target_q = reward + self.gamma * next_q_val * (1 - terminal)\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = self.loss_ftn(q.squeeze(), target_q.squeeze().detach())\n",
    "        ################################################################\n",
    "\n",
    "        # Update Critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        ################################################################\n",
    "        # Task 4: Complete the actor loss calculation\n",
    "        mu_s = self.actor(state)\n",
    "        actor_loss = - self.critic(state, mu_s).mean()\n",
    "#         actor_loss = - torch.sum(self.critic(state, action)) / self.batch_size\n",
    "        ################################################################\n",
    "\n",
    "        # Update Actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "#         if self.num_fit % 100 == 0:\n",
    "#             self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "#             self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.update_target(self.critic, self.critic_target, tau=self.tau)\n",
    "        self.update_target(self.actor, self.actor_target, tau=self.tau)\n",
    "\n",
    "        return critic_loss.item(), actor_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :    0, reward : -341.622, critic loss : 1.619, actor loss : 0.787\n",
      "episode :    1, reward : -411.746, critic loss : 67.707, actor loss : 3.536\n",
      "episode :    2, reward : -13.459, critic loss : 84.022, actor loss : 4.081\n",
      "episode :    3, reward : -721.306, critic loss : 74.299, actor loss : 3.389\n",
      "episode :    4, reward : -440.273, critic loss : 110.380, actor loss : 4.462\n",
      "episode :    5, reward : -539.688, critic loss : 79.051, actor loss : 6.066\n",
      "episode :    6, reward : -391.826, critic loss : 74.172, actor loss : 6.780\n",
      "episode :    7, reward : -137.406, critic loss : 69.020, actor loss : 7.345\n",
      "episode :    8, reward : -122.095, critic loss : 81.592, actor loss : 7.447\n",
      "episode :    9, reward : -187.629, critic loss : 76.445, actor loss : 8.079\n",
      "episode :   10, reward : -204.566, critic loss : 62.537, actor loss : 9.246\n",
      "episode :   11, reward : -149.620, critic loss : 49.390, actor loss : 9.537\n",
      "episode :   12, reward : -128.614, critic loss : 60.195, actor loss : 10.309\n",
      "episode :   13, reward : -216.589, critic loss : 63.873, actor loss : 11.153\n",
      "episode :   14, reward : -140.339, critic loss : 50.771, actor loss : 11.142\n",
      "episode :   15, reward : -124.869, critic loss : 38.141, actor loss : 11.697\n",
      "episode :   16, reward : -117.234, critic loss : 40.623, actor loss : 13.111\n",
      "episode :   17, reward : -114.980, critic loss : 30.124, actor loss : 12.387\n",
      "episode :   18, reward : -126.816, critic loss : 27.571, actor loss : 12.427\n",
      "episode :   19, reward : -706.324, critic loss : 18.960, actor loss : 12.915\n",
      "episode :   20, reward : -573.268, critic loss : 20.662, actor loss : 13.414\n",
      "episode :   21, reward : -1264.330, critic loss : 18.411, actor loss : 14.580\n",
      "episode :   22, reward : -2223.103, critic loss : 16.840, actor loss : 20.314\n",
      "episode :   23, reward : -134.521, critic loss : 13.453, actor loss : 26.227\n",
      "episode :   24, reward : -19.142, critic loss : 11.496, actor loss : 25.929\n",
      "episode :   25, reward : -108.875, critic loss : 11.073, actor loss : 26.811\n",
      "episode :   26, reward : -25.468, critic loss : 13.023, actor loss : 25.653\n",
      "episode :   27, reward : -54.711, critic loss : 12.488, actor loss : 23.613\n",
      "episode :   28, reward : -74.231, critic loss : 11.304, actor loss : 18.238\n",
      "episode :   29, reward :  11.201, critic loss : 11.782, actor loss : 11.987\n",
      "episode :   30, reward : -170.113, critic loss : 9.398, actor loss : 8.851\n",
      "episode :   31, reward : -68.729, critic loss : 9.303, actor loss : 5.402\n",
      "episode :   32, reward : -119.215, critic loss : 11.863, actor loss : 1.881\n",
      "episode :   33, reward : -63.081, critic loss : 10.948, actor loss : -1.898\n",
      "episode :   34, reward : -23.401, critic loss : 11.237, actor loss : -5.764\n",
      "episode :   35, reward :  29.652, critic loss : 19.979, actor loss : -10.005\n",
      "episode :   36, reward : -82.500, critic loss : 11.464, actor loss : -14.337\n",
      "episode :   37, reward : -29.794, critic loss : 7.042, actor loss : -17.653\n",
      "episode :   38, reward : -42.586, critic loss : 4.381, actor loss : -22.839\n",
      "episode :   39, reward : -47.491, critic loss : 4.435, actor loss : -23.573\n",
      "episode :   40, reward : -25.224, critic loss : 4.303, actor loss : -25.089\n",
      "episode :   41, reward : -46.208, critic loss : 3.810, actor loss : -26.621\n",
      "episode :   42, reward : -41.438, critic loss : 1.408, actor loss : -27.652\n",
      "episode :   43, reward : -4.297, critic loss : 1.156, actor loss : -27.309\n",
      "episode :   44, reward : -25.397, critic loss : 1.242, actor loss : -27.146\n",
      "episode :   45, reward : -37.296, critic loss : 1.192, actor loss : -27.800\n",
      "episode :   46, reward : -117.692, critic loss : 1.100, actor loss : -28.091\n",
      "episode :   47, reward : -206.063, critic loss : 0.914, actor loss : -27.770\n",
      "episode :   48, reward : -96.066, critic loss : 4.052, actor loss : -27.892\n",
      "episode :   49, reward : -113.895, critic loss : 3.859, actor loss : -27.533\n",
      "episode :   50, reward : -41.330, critic loss : 2.093, actor loss : -27.252\n",
      "episode :   51, reward : -134.917, critic loss : 4.057, actor loss : -26.899\n",
      "episode :   52, reward : -79.438, critic loss : 1.243, actor loss : -27.124\n",
      "episode :   53, reward : -103.230, critic loss : 13.720, actor loss : -27.261\n",
      "episode :   54, reward : -64.785, critic loss : 8.187, actor loss : -27.344\n",
      "episode :   55, reward : -28.539, critic loss : 9.234, actor loss : -26.448\n",
      "episode :   56, reward : -73.696, critic loss : 9.730, actor loss : -25.826\n",
      "episode :   57, reward : -31.483, critic loss : 5.552, actor loss : -25.831\n",
      "episode :   58, reward : -43.391, critic loss : 8.175, actor loss : -25.522\n",
      "episode :   59, reward :  2.254, critic loss : 6.151, actor loss : -24.939\n",
      "episode :   60, reward : -35.966, critic loss : 7.875, actor loss : -24.047\n",
      "episode :   61, reward :  67.630, critic loss : 6.001, actor loss : -23.587\n",
      "episode :   62, reward :  44.044, critic loss : 6.552, actor loss : -24.165\n",
      "episode :   63, reward :  166.030, critic loss : 6.495, actor loss : -25.005\n",
      "episode :   64, reward :  162.563, critic loss : 5.595, actor loss : -25.577\n",
      "episode :   65, reward :  250.487, critic loss : 8.144, actor loss : -26.396\n",
      "episode :   66, reward :  135.675, critic loss : 4.972, actor loss : -26.808\n",
      "episode :   67, reward :  208.998, critic loss : 5.076, actor loss : -27.848\n",
      "episode :   68, reward :  228.132, critic loss : 3.050, actor loss : -28.708\n",
      "episode :   69, reward :  280.227, critic loss : 4.565, actor loss : -30.362\n",
      "episode :   70, reward :  6.842, critic loss : 11.817, actor loss : -31.767\n",
      "episode :   71, reward :  198.698, critic loss : 12.048, actor loss : -33.108\n",
      "episode :   72, reward :  265.407, critic loss : 9.541, actor loss : -34.239\n",
      "episode :   73, reward :  1.905, critic loss : 9.122, actor loss : -35.527\n",
      "episode :   74, reward :  205.500, critic loss : 9.727, actor loss : -35.548\n",
      "episode :   75, reward :  12.094, critic loss : 6.947, actor loss : -36.391\n",
      "episode :   76, reward :  9.565, critic loss : 10.451, actor loss : -37.183\n",
      "episode :   77, reward : -98.147, critic loss : 9.189, actor loss : -37.989\n",
      "episode :   78, reward : -57.716, critic loss : 12.114, actor loss : -39.399\n",
      "episode :   79, reward :  221.550, critic loss : 20.486, actor loss : -39.201\n",
      "episode :   80, reward :  178.758, critic loss : 16.362, actor loss : -40.135\n",
      "episode :   81, reward :  278.969, critic loss : 20.262, actor loss : -40.513\n",
      "episode :   82, reward : -14.002, critic loss : 16.379, actor loss : -41.258\n",
      "episode :   83, reward :  198.778, critic loss : 19.426, actor loss : -41.789\n",
      "episode :   84, reward : -18.476, critic loss : 17.479, actor loss : -41.705\n",
      "episode :   85, reward :  20.484, critic loss : 14.658, actor loss : -42.676\n",
      "episode :   86, reward : -99.458, critic loss : 19.124, actor loss : -42.708\n",
      "episode :   87, reward : -6.177, critic loss : 31.011, actor loss : -42.899\n",
      "episode :   88, reward :  202.193, critic loss : 22.896, actor loss : -44.192\n",
      "episode :   89, reward : -6.673, critic loss : 14.887, actor loss : -44.781\n",
      "episode :   90, reward : -51.448, critic loss : 26.091, actor loss : -45.026\n",
      "episode :   91, reward :  250.816, critic loss : 23.295, actor loss : -45.855\n",
      "episode :   92, reward : -53.228, critic loss : 28.945, actor loss : -45.440\n",
      "episode :   93, reward :  44.638, critic loss : 27.251, actor loss : -46.551\n",
      "episode :   94, reward :  222.071, critic loss : 24.191, actor loss : -46.198\n",
      "episode :   95, reward : -37.288, critic loss : 19.083, actor loss : -46.212\n",
      "episode :   96, reward : -41.403, critic loss : 24.158, actor loss : -46.854\n",
      "episode :   97, reward : -17.514, critic loss : 32.827, actor loss : -47.023\n",
      "episode :   98, reward : -98.614, critic loss : 32.158, actor loss : -46.661\n",
      "episode :   99, reward :  159.967, critic loss : 32.414, actor loss : -45.977\n",
      "episode :  100, reward :  206.226, critic loss : 25.632, actor loss : -47.002\n",
      "episode :  101, reward :  226.621, critic loss : 28.499, actor loss : -47.523\n",
      "episode :  102, reward :  209.274, critic loss : 31.498, actor loss : -47.079\n",
      "episode :  103, reward :  198.479, critic loss : 24.813, actor loss : -46.509\n",
      "episode :  104, reward :  173.563, critic loss : 23.798, actor loss : -46.075\n",
      "episode :  105, reward : -73.012, critic loss : 22.511, actor loss : -46.713\n",
      "episode :  106, reward : -38.078, critic loss : 22.207, actor loss : -45.167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  107, reward : -6.214, critic loss : 18.774, actor loss : -44.215\n",
      "episode :  108, reward : -52.417, critic loss : 13.654, actor loss : -43.490\n",
      "episode :  109, reward : -95.683, critic loss : 11.701, actor loss : -42.136\n",
      "episode :  110, reward : -96.242, critic loss : 10.113, actor loss : -41.112\n",
      "episode :  111, reward : -84.339, critic loss : 6.645, actor loss : -40.723\n",
      "episode :  112, reward : -52.508, critic loss : 4.361, actor loss : -40.564\n",
      "episode :  113, reward : -96.407, critic loss : 5.301, actor loss : -40.237\n",
      "episode :  114, reward :  246.987, critic loss : 4.398, actor loss : -39.253\n",
      "episode :  115, reward : -64.165, critic loss : 3.859, actor loss : -38.415\n",
      "episode :  116, reward :  130.720, critic loss : 4.347, actor loss : -38.071\n",
      "episode :  117, reward : -136.406, critic loss : 3.671, actor loss : -38.488\n",
      "episode :  118, reward : -73.516, critic loss : 2.729, actor loss : -38.054\n",
      "episode :  119, reward : -162.804, critic loss : 7.401, actor loss : -38.894\n",
      "episode :  120, reward : -58.601, critic loss : 11.105, actor loss : -38.336\n",
      "episode :  121, reward : -66.137, critic loss : 6.923, actor loss : -38.483\n",
      "episode :  122, reward : -67.898, critic loss : 13.441, actor loss : -38.538\n",
      "episode :  123, reward :  125.186, critic loss : 15.624, actor loss : -39.208\n",
      "episode :  124, reward : -49.512, critic loss : 11.777, actor loss : -38.762\n",
      "episode :  125, reward : -113.876, critic loss : 15.491, actor loss : -38.383\n",
      "episode :  126, reward : -64.796, critic loss : 11.077, actor loss : -38.349\n",
      "episode :  127, reward : -27.222, critic loss : 11.324, actor loss : -38.478\n",
      "episode :  128, reward :  242.700, critic loss : 9.066, actor loss : -38.245\n",
      "episode :  129, reward : -26.033, critic loss : 12.967, actor loss : -38.140\n",
      "episode :  130, reward :  203.534, critic loss : 8.187, actor loss : -37.700\n",
      "episode :  131, reward :  222.405, critic loss : 11.083, actor loss : -38.286\n",
      "episode :  132, reward : -15.765, critic loss : 9.429, actor loss : -36.624\n",
      "episode :  133, reward : -116.948, critic loss : 10.491, actor loss : -37.056\n",
      "episode :  134, reward : -169.836, critic loss : 9.957, actor loss : -37.216\n",
      "episode :  135, reward : -128.622, critic loss : 16.992, actor loss : -37.189\n",
      "episode :  136, reward : -165.916, critic loss : 11.925, actor loss : -36.361\n",
      "episode :  137, reward : -102.278, critic loss : 13.349, actor loss : -35.793\n",
      "episode :  138, reward : -126.570, critic loss : 11.316, actor loss : -35.197\n",
      "episode :  139, reward : -95.993, critic loss : 8.045, actor loss : -34.498\n",
      "episode :  140, reward : -93.706, critic loss : 10.153, actor loss : -34.130\n",
      "episode :  141, reward : -116.903, critic loss : 7.528, actor loss : -33.463\n",
      "episode :  142, reward : -122.948, critic loss : 7.486, actor loss : -32.186\n",
      "episode :  143, reward : -84.686, critic loss : 7.793, actor loss : -31.494\n",
      "episode :  144, reward : -145.879, critic loss : 9.181, actor loss : -30.826\n",
      "episode :  145, reward : -105.303, critic loss : 10.765, actor loss : -30.322\n",
      "episode :  146, reward :  194.080, critic loss : 10.711, actor loss : -29.697\n",
      "episode :  147, reward : -123.410, critic loss : 6.982, actor loss : -29.637\n",
      "episode :  148, reward :  159.344, critic loss : 12.975, actor loss : -29.251\n",
      "episode :  149, reward : -117.652, critic loss : 12.916, actor loss : -29.114\n",
      "episode :  150, reward : -111.617, critic loss : 9.046, actor loss : -28.512\n",
      "episode :  151, reward : -181.236, critic loss : 8.373, actor loss : -28.552\n",
      "episode :  152, reward : -100.055, critic loss : 11.273, actor loss : -28.391\n",
      "episode :  153, reward : -71.517, critic loss : 12.853, actor loss : -29.228\n",
      "episode :  154, reward : -126.746, critic loss : 15.786, actor loss : -29.017\n",
      "episode :  155, reward :  188.280, critic loss : 11.928, actor loss : -29.754\n",
      "episode :  156, reward : -33.438, critic loss : 17.204, actor loss : -30.327\n",
      "episode :  157, reward :  196.789, critic loss : 16.812, actor loss : -31.193\n",
      "episode :  158, reward :  181.195, critic loss : 13.007, actor loss : -32.360\n",
      "episode :  159, reward :  197.202, critic loss : 14.090, actor loss : -34.069\n",
      "episode :  160, reward : -46.859, critic loss : 12.000, actor loss : -34.647\n",
      "episode :  161, reward :  199.155, critic loss : 11.555, actor loss : -35.398\n",
      "episode :  162, reward : -45.304, critic loss : 11.257, actor loss : -36.413\n",
      "episode :  163, reward : -55.560, critic loss : 11.829, actor loss : -36.029\n",
      "episode :  164, reward :  242.008, critic loss : 13.964, actor loss : -37.698\n",
      "episode :  165, reward :  166.754, critic loss : 14.636, actor loss : -38.662\n",
      "episode :  166, reward :  203.950, critic loss : 11.969, actor loss : -38.825\n",
      "episode :  167, reward :  281.861, critic loss : 16.632, actor loss : -40.038\n",
      "episode :  168, reward :  202.477, critic loss : 12.324, actor loss : -40.885\n",
      "episode :  169, reward :  24.435, critic loss : 15.448, actor loss : -41.780\n",
      "episode :  170, reward : -125.582, critic loss : 9.156, actor loss : -42.178\n",
      "episode :  171, reward : -109.062, critic loss : 12.111, actor loss : -42.071\n",
      "episode :  172, reward : -45.653, critic loss : 21.144, actor loss : -41.370\n",
      "episode :  173, reward : -44.271, critic loss : 19.107, actor loss : -41.622\n",
      "episode :  174, reward : -58.324, critic loss : 16.006, actor loss : -43.021\n",
      "episode :  175, reward : -59.624, critic loss : 9.371, actor loss : -43.455\n",
      "episode :  176, reward : -49.068, critic loss : 24.906, actor loss : -43.781\n",
      "episode :  177, reward : -157.207, critic loss : 14.360, actor loss : -43.861\n",
      "episode :  178, reward : -172.052, critic loss : 28.889, actor loss : -43.628\n",
      "episode :  179, reward : -50.237, critic loss : 19.643, actor loss : -43.157\n",
      "episode :  180, reward : -46.632, critic loss : 25.186, actor loss : -44.426\n",
      "episode :  181, reward : -122.053, critic loss : 23.120, actor loss : -43.302\n",
      "episode :  182, reward :  9.944, critic loss : 24.297, actor loss : -44.497\n",
      "episode :  183, reward : -18.716, critic loss : 20.777, actor loss : -44.995\n",
      "episode :  184, reward :  250.611, critic loss : 18.222, actor loss : -45.140\n",
      "episode :  185, reward :  47.491, critic loss : 24.728, actor loss : -46.422\n",
      "episode :  186, reward : -127.722, critic loss : 23.460, actor loss : -45.201\n",
      "episode :  187, reward : -53.388, critic loss : 19.617, actor loss : -44.493\n",
      "episode :  188, reward : -67.894, critic loss : 17.086, actor loss : -44.547\n",
      "episode :  189, reward : -264.727, critic loss : 14.681, actor loss : -44.860\n",
      "episode :  190, reward : -145.821, critic loss : 18.268, actor loss : -45.979\n",
      "episode :  191, reward : -180.343, critic loss : 11.892, actor loss : -45.752\n",
      "episode :  192, reward : -188.470, critic loss : 13.116, actor loss : -46.329\n",
      "episode :  193, reward : -197.520, critic loss : 14.589, actor loss : -46.450\n",
      "episode :  194, reward : -216.185, critic loss : 11.218, actor loss : -46.034\n",
      "episode :  195, reward : -210.667, critic loss : 12.013, actor loss : -46.298\n",
      "episode :  196, reward : -209.501, critic loss : 11.957, actor loss : -45.905\n",
      "episode :  197, reward : -215.644, critic loss : 12.908, actor loss : -46.263\n",
      "episode :  198, reward : -263.237, critic loss : 8.641, actor loss : -45.480\n",
      "episode :  199, reward : -235.076, critic loss : 10.224, actor loss : -44.602\n",
      "episode :  200, reward : -255.562, critic loss : 9.786, actor loss : -44.020\n",
      "episode :  201, reward : -243.081, critic loss : 9.894, actor loss : -43.676\n",
      "episode :  202, reward : -237.160, critic loss : 13.026, actor loss : -42.785\n",
      "episode :  203, reward : -251.670, critic loss : 9.392, actor loss : -41.800\n",
      "episode :  204, reward : -250.635, critic loss : 7.687, actor loss : -40.702\n",
      "episode :  205, reward : -65.802, critic loss : 7.185, actor loss : -39.609\n",
      "episode :  206, reward : -90.522, critic loss : 6.059, actor loss : -37.958\n",
      "episode :  207, reward : -136.781, critic loss : 8.643, actor loss : -36.662\n",
      "episode :  208, reward : -224.995, critic loss : 8.399, actor loss : -35.887\n",
      "episode :  209, reward : -212.351, critic loss : 8.843, actor loss : -35.031\n",
      "episode :  210, reward : -174.881, critic loss : 8.193, actor loss : -31.417\n",
      "episode :  211, reward : -159.975, critic loss : 5.959, actor loss : -29.919\n",
      "episode :  212, reward : -144.930, critic loss : 7.685, actor loss : -30.217\n",
      "episode :  213, reward : -181.560, critic loss : 7.146, actor loss : -28.989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  214, reward : -191.020, critic loss : 5.670, actor loss : -27.593\n",
      "episode :  215, reward : -81.040, critic loss : 4.649, actor loss : -27.028\n",
      "episode :  216, reward : -152.049, critic loss : 7.354, actor loss : -26.201\n",
      "episode :  217, reward : -196.889, critic loss : 4.866, actor loss : -25.990\n",
      "episode :  218, reward : -142.111, critic loss : 8.665, actor loss : -25.500\n",
      "episode :  219, reward : -189.121, critic loss : 6.377, actor loss : -25.611\n",
      "episode :  220, reward : -87.399, critic loss : 5.963, actor loss : -24.893\n",
      "episode :  221, reward : -186.915, critic loss : 6.708, actor loss : -24.939\n",
      "episode :  222, reward : -101.884, critic loss : 5.229, actor loss : -24.124\n",
      "episode :  223, reward : -149.923, critic loss : 6.026, actor loss : -24.055\n",
      "episode :  224, reward : -184.488, critic loss : 7.669, actor loss : -21.874\n",
      "episode :  225, reward : -188.740, critic loss : 6.545, actor loss : -21.990\n",
      "episode :  226, reward : -76.479, critic loss : 5.100, actor loss : -20.612\n",
      "episode :  227, reward : -151.531, critic loss : 6.684, actor loss : -20.074\n",
      "episode :  228, reward : -167.713, critic loss : 4.686, actor loss : -19.059\n",
      "episode :  229, reward : -185.528, critic loss : 4.707, actor loss : -18.967\n",
      "episode :  230, reward : -148.817, critic loss : 5.416, actor loss : -19.009\n",
      "episode :  231, reward : -143.694, critic loss : 4.677, actor loss : -17.539\n",
      "episode :  232, reward : -99.240, critic loss : 5.093, actor loss : -16.575\n",
      "episode :  233, reward : -155.000, critic loss : 7.593, actor loss : -15.797\n",
      "episode :  234, reward : -125.041, critic loss : 4.617, actor loss : -15.259\n",
      "episode :  235, reward : -152.318, critic loss : 6.101, actor loss : -14.652\n",
      "episode :  236, reward : -153.416, critic loss : 7.109, actor loss : -16.705\n",
      "episode :  237, reward : -127.786, critic loss : 4.213, actor loss : -15.212\n",
      "episode :  238, reward : -133.524, critic loss : 5.960, actor loss : -15.548\n",
      "episode :  239, reward : -145.689, critic loss : 5.358, actor loss : -14.636\n",
      "episode :  240, reward : -184.996, critic loss : 6.063, actor loss : -15.713\n",
      "episode :  241, reward : -123.363, critic loss : 4.751, actor loss : -15.257\n",
      "episode :  242, reward : -83.102, critic loss : 5.853, actor loss : -13.170\n",
      "episode :  243, reward : -41.915, critic loss : 6.664, actor loss : -14.566\n",
      "episode :  244, reward : -120.043, critic loss : 3.899, actor loss : -16.519\n",
      "episode :  245, reward : -88.757, critic loss : 7.650, actor loss : -19.671\n",
      "episode :  246, reward : -68.184, critic loss : 5.993, actor loss : -25.152\n",
      "episode :  247, reward : -79.050, critic loss : 7.554, actor loss : -27.142\n",
      "episode :  248, reward : -46.943, critic loss : 6.327, actor loss : -28.216\n",
      "episode :  249, reward : -49.777, critic loss : 6.809, actor loss : -33.158\n",
      "episode :  250, reward :  38.680, critic loss : 7.982, actor loss : -40.596\n",
      "episode :  251, reward : -130.299, critic loss : 9.815, actor loss : -44.885\n",
      "episode :  252, reward : -104.891, critic loss : 5.586, actor loss : -45.108\n",
      "episode :  253, reward : -111.365, critic loss : 7.267, actor loss : -45.890\n",
      "episode :  254, reward : -148.815, critic loss : 7.324, actor loss : -47.080\n",
      "episode :  255, reward : -125.371, critic loss : 6.893, actor loss : -46.853\n",
      "episode :  256, reward : -148.768, critic loss : 7.796, actor loss : -46.421\n",
      "episode :  257, reward : -229.353, critic loss : 9.380, actor loss : -48.023\n",
      "episode :  258, reward : -35.236, critic loss : 7.113, actor loss : -47.547\n",
      "episode :  259, reward :  82.570, critic loss : 6.600, actor loss : -50.510\n",
      "episode :  260, reward : -11.407, critic loss : 6.358, actor loss : -49.790\n",
      "episode :  261, reward : -57.651, critic loss : 5.987, actor loss : -48.927\n",
      "episode :  262, reward : -58.470, critic loss : 4.604, actor loss : -49.621\n",
      "episode :  263, reward : -46.018, critic loss : 4.367, actor loss : -49.164\n",
      "episode :  264, reward : -11.056, critic loss : 4.451, actor loss : -48.916\n",
      "episode :  265, reward :  133.027, critic loss : 4.856, actor loss : -48.982\n",
      "episode :  266, reward :  110.978, critic loss : 4.505, actor loss : -51.658\n",
      "episode :  267, reward :  2.860, critic loss : 4.461, actor loss : -54.508\n",
      "episode :  268, reward : -14.850, critic loss : 3.771, actor loss : -55.076\n",
      "episode :  269, reward :  39.584, critic loss : 3.432, actor loss : -53.259\n",
      "episode :  270, reward : -47.183, critic loss : 2.736, actor loss : -51.538\n",
      "episode :  271, reward : -34.747, critic loss : 3.346, actor loss : -49.655\n",
      "episode :  272, reward :  117.326, critic loss : 2.927, actor loss : -48.083\n",
      "episode :  273, reward : -52.576, critic loss : 2.295, actor loss : -46.629\n",
      "episode :  274, reward :  76.732, critic loss : 2.633, actor loss : -44.786\n",
      "episode :  275, reward :  89.764, critic loss : 2.637, actor loss : -43.011\n",
      "episode :  276, reward :  65.400, critic loss : 2.513, actor loss : -41.850\n",
      "episode :  277, reward : -4.194, critic loss : 2.679, actor loss : -41.011\n",
      "episode :  278, reward : -76.266, critic loss : 2.056, actor loss : -41.015\n",
      "episode :  279, reward : -21.465, critic loss : 2.023, actor loss : -40.882\n",
      "episode :  280, reward : -69.584, critic loss : 1.889, actor loss : -40.140\n",
      "episode :  281, reward : -105.249, critic loss : 1.838, actor loss : -39.919\n",
      "episode :  282, reward : -71.458, critic loss : 1.732, actor loss : -40.337\n",
      "episode :  283, reward : -29.633, critic loss : 2.088, actor loss : -39.915\n",
      "episode :  284, reward : -52.062, critic loss : 2.223, actor loss : -36.957\n",
      "episode :  285, reward : -15.081, critic loss : 2.187, actor loss : -34.085\n",
      "episode :  286, reward : -46.321, critic loss : 2.441, actor loss : -30.958\n",
      "episode :  287, reward : -24.781, critic loss : 2.201, actor loss : -27.195\n",
      "episode :  288, reward : -35.322, critic loss : 1.717, actor loss : -23.631\n",
      "episode :  289, reward : -41.588, critic loss : 1.485, actor loss : -20.319\n",
      "episode :  290, reward : -39.132, critic loss : 1.192, actor loss : -17.072\n",
      "episode :  291, reward : -19.264, critic loss : 0.996, actor loss : -15.076\n",
      "episode :  292, reward :  146.919, critic loss : 0.974, actor loss : -13.417\n",
      "episode :  293, reward :  180.763, critic loss : 1.760, actor loss : -12.947\n",
      "episode :  294, reward : -63.007, critic loss : 2.670, actor loss : -12.680\n",
      "episode :  295, reward :  144.560, critic loss : 3.434, actor loss : -13.297\n",
      "episode :  296, reward :  36.929, critic loss : 3.866, actor loss : -14.201\n",
      "episode :  297, reward :  164.794, critic loss : 4.096, actor loss : -14.474\n",
      "episode :  298, reward :  133.081, critic loss : 3.969, actor loss : -14.988\n",
      "episode :  299, reward :  238.300, critic loss : 5.494, actor loss : -15.568\n",
      "episode :  300, reward : -42.749, critic loss : 6.486, actor loss : -16.858\n",
      "episode :  301, reward : -54.287, critic loss : 7.348, actor loss : -17.596\n",
      "episode :  302, reward :  218.993, critic loss : 4.889, actor loss : -18.091\n",
      "episode :  303, reward :  36.786, critic loss : 8.003, actor loss : -18.178\n",
      "episode :  304, reward :  163.558, critic loss : 5.960, actor loss : -20.768\n",
      "episode :  305, reward :  16.120, critic loss : 6.090, actor loss : -21.426\n",
      "episode :  306, reward :  21.180, critic loss : 7.785, actor loss : -23.200\n",
      "episode :  307, reward :  21.125, critic loss : 7.692, actor loss : -24.355\n",
      "episode :  308, reward : -8.287, critic loss : 5.938, actor loss : -25.708\n",
      "episode :  309, reward :  10.690, critic loss : 9.392, actor loss : -25.693\n",
      "episode :  310, reward : -38.597, critic loss : 9.041, actor loss : -26.494\n",
      "episode :  311, reward : -27.759, critic loss : 9.075, actor loss : -25.835\n",
      "episode :  312, reward : -0.911, critic loss : 10.168, actor loss : -25.787\n",
      "episode :  313, reward :  218.953, critic loss : 8.901, actor loss : -27.536\n",
      "episode :  314, reward :  10.214, critic loss : 13.075, actor loss : -28.545\n",
      "episode :  315, reward :  260.109, critic loss : 11.811, actor loss : -29.579\n",
      "episode :  316, reward :  254.886, critic loss : 11.374, actor loss : -28.930\n",
      "episode :  317, reward :  33.741, critic loss : 12.313, actor loss : -30.202\n",
      "episode :  318, reward :  302.625, critic loss : 16.689, actor loss : -30.977\n",
      "episode :  319, reward :  280.890, critic loss : 13.154, actor loss : -32.010\n",
      "episode :  320, reward :  229.322, critic loss : 15.587, actor loss : -33.229\n",
      "episode :  321, reward :  134.877, critic loss : 11.407, actor loss : -33.268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  322, reward :  29.629, critic loss : 11.558, actor loss : -34.232\n",
      "episode :  323, reward :  205.701, critic loss : 16.757, actor loss : -34.830\n",
      "episode :  324, reward :  2.752, critic loss : 12.414, actor loss : -36.286\n",
      "episode :  325, reward :  245.259, critic loss : 17.742, actor loss : -37.861\n",
      "episode :  326, reward :  201.113, critic loss : 14.986, actor loss : -39.396\n",
      "episode :  327, reward :  224.339, critic loss : 16.153, actor loss : -39.897\n",
      "episode :  328, reward :  287.667, critic loss : 18.521, actor loss : -39.920\n",
      "episode :  329, reward :  266.263, critic loss : 14.534, actor loss : -40.818\n",
      "episode :  330, reward :  303.717, critic loss : 15.986, actor loss : -41.948\n",
      "episode :  331, reward :  294.616, critic loss : 18.320, actor loss : -43.478\n",
      "episode :  332, reward :  281.605, critic loss : 16.651, actor loss : -43.642\n",
      "episode :  333, reward :  286.219, critic loss : 16.406, actor loss : -44.404\n",
      "episode :  334, reward :  280.722, critic loss : 13.340, actor loss : -46.339\n",
      "episode :  335, reward :  201.962, critic loss : 16.289, actor loss : -47.804\n",
      "episode :  336, reward :  219.485, critic loss : 17.831, actor loss : -49.772\n",
      "episode :  337, reward :  219.036, critic loss : 16.106, actor loss : -51.195\n",
      "episode :  338, reward :  50.619, critic loss : 16.496, actor loss : -52.974\n",
      "episode :  339, reward :  222.836, critic loss : 17.635, actor loss : -53.803\n",
      "episode :  340, reward :  211.463, critic loss : 12.128, actor loss : -53.176\n",
      "episode :  341, reward :  237.051, critic loss : 11.951, actor loss : -54.540\n",
      "episode :  342, reward :  199.007, critic loss : 12.213, actor loss : -55.796\n",
      "episode :  343, reward :  207.271, critic loss : 11.057, actor loss : -57.725\n",
      "episode :  344, reward :  227.442, critic loss : 9.660, actor loss : -58.214\n",
      "episode :  345, reward :  225.248, critic loss : 8.227, actor loss : -58.359\n",
      "episode :  346, reward :  196.137, critic loss : 8.152, actor loss : -59.240\n",
      "episode :  347, reward :  219.106, critic loss : 8.585, actor loss : -59.645\n",
      "episode :  348, reward :  274.240, critic loss : 8.183, actor loss : -60.275\n",
      "episode :  349, reward :  225.947, critic loss : 7.600, actor loss : -59.902\n",
      "episode :  350, reward :  191.607, critic loss : 7.249, actor loss : -59.699\n",
      "episode :  351, reward :  221.626, critic loss : 6.840, actor loss : -59.579\n",
      "episode :  352, reward :  279.628, critic loss : 7.852, actor loss : -59.595\n",
      "episode :  353, reward :  250.984, critic loss : 5.965, actor loss : -59.985\n",
      "episode :  354, reward :  281.737, critic loss : 5.304, actor loss : -60.869\n",
      "episode :  355, reward :  199.095, critic loss : 6.849, actor loss : -61.295\n",
      "episode :  356, reward :  216.508, critic loss : 6.033, actor loss : -61.898\n",
      "episode :  357, reward :  256.925, critic loss : 5.922, actor loss : -62.098\n",
      "episode :  358, reward :  239.123, critic loss : 5.482, actor loss : -62.689\n",
      "episode :  359, reward :  150.172, critic loss : 5.380, actor loss : -62.636\n",
      "episode :  360, reward :  214.349, critic loss : 5.404, actor loss : -62.638\n",
      "episode :  361, reward :  124.132, critic loss : 4.877, actor loss : -62.850\n",
      "episode :  362, reward :  206.227, critic loss : 3.861, actor loss : -62.510\n",
      "episode :  363, reward :  251.268, critic loss : 5.618, actor loss : -62.357\n",
      "episode :  364, reward :  183.874, critic loss : 4.495, actor loss : -62.474\n",
      "episode :  365, reward :  202.439, critic loss : 4.325, actor loss : -62.537\n",
      "episode :  366, reward :  2.386, critic loss : 4.389, actor loss : -62.421\n",
      "episode :  367, reward :  208.383, critic loss : 4.118, actor loss : -63.600\n",
      "episode :  368, reward :  19.026, critic loss : 7.692, actor loss : -63.370\n",
      "episode :  369, reward :  231.929, critic loss : 14.288, actor loss : -63.879\n",
      "episode :  370, reward : -499.618, critic loss : 10.149, actor loss : -64.237\n",
      "episode :  371, reward : -161.305, critic loss : 10.855, actor loss : -64.185\n",
      "episode :  372, reward : -32.253, critic loss : 9.647, actor loss : -63.041\n",
      "episode :  373, reward :  174.030, critic loss : 10.428, actor loss : -61.806\n",
      "episode :  374, reward : -18.412, critic loss : 2.355, actor loss : -61.800\n",
      "episode :  375, reward :  267.068, critic loss : 6.812, actor loss : -61.463\n",
      "episode :  376, reward :  136.100, critic loss : 10.874, actor loss : -61.012\n",
      "episode :  377, reward :  228.302, critic loss : 10.719, actor loss : -60.625\n",
      "episode :  378, reward :  236.844, critic loss : 9.592, actor loss : -60.995\n",
      "episode :  379, reward : -50.619, critic loss : 8.373, actor loss : -61.546\n",
      "episode :  380, reward : -19.223, critic loss : 5.801, actor loss : -61.777\n",
      "episode :  381, reward :  229.755, critic loss : 9.349, actor loss : -61.470\n",
      "episode :  382, reward :  90.822, critic loss : 9.802, actor loss : -62.056\n",
      "episode :  383, reward : -46.392, critic loss : 11.362, actor loss : -61.749\n",
      "episode :  384, reward :  4.496, critic loss : 8.664, actor loss : -59.572\n",
      "episode :  385, reward :  138.914, critic loss : 10.652, actor loss : -58.602\n",
      "episode :  386, reward : -30.963, critic loss : 7.706, actor loss : -58.970\n",
      "episode :  387, reward :  149.690, critic loss : 10.586, actor loss : -58.206\n",
      "episode :  388, reward :  136.728, critic loss : 10.411, actor loss : -57.358\n",
      "episode :  389, reward :  164.908, critic loss : 11.521, actor loss : -57.768\n",
      "episode :  390, reward : -65.583, critic loss : 7.876, actor loss : -57.657\n",
      "episode :  391, reward : -9.094, critic loss : 7.936, actor loss : -56.832\n",
      "episode :  392, reward : -98.036, critic loss : 6.842, actor loss : -54.841\n",
      "episode :  393, reward :  205.880, critic loss : 6.129, actor loss : -53.475\n",
      "episode :  394, reward : -158.886, critic loss : 4.110, actor loss : -52.807\n",
      "episode :  395, reward : -235.638, critic loss : 10.308, actor loss : -52.558\n",
      "episode :  396, reward :  129.778, critic loss : 9.532, actor loss : -51.463\n",
      "episode :  397, reward :  154.892, critic loss : 8.810, actor loss : -53.632\n",
      "episode :  398, reward :  217.850, critic loss : 8.830, actor loss : -54.667\n",
      "episode :  399, reward :  219.526, critic loss : 9.808, actor loss : -55.653\n",
      "episode :  400, reward : -42.044, critic loss : 8.844, actor loss : -55.846\n",
      "episode :  401, reward : -48.472, critic loss : 5.540, actor loss : -54.099\n",
      "episode :  402, reward : -32.304, critic loss : 6.199, actor loss : -51.917\n",
      "episode :  403, reward : -17.453, critic loss : 4.532, actor loss : -51.459\n",
      "episode :  404, reward : -38.380, critic loss : 4.896, actor loss : -51.989\n",
      "episode :  405, reward : -51.238, critic loss : 5.223, actor loss : -51.789\n",
      "episode :  406, reward :  173.965, critic loss : 6.221, actor loss : -51.246\n",
      "episode :  407, reward :  250.342, critic loss : 5.709, actor loss : -51.764\n",
      "episode :  408, reward : -24.513, critic loss : 4.150, actor loss : -52.563\n",
      "episode :  409, reward :  27.659, critic loss : 3.258, actor loss : -51.145\n",
      "episode :  410, reward :  218.076, critic loss : 3.500, actor loss : -50.447\n",
      "episode :  411, reward :  145.298, critic loss : 3.748, actor loss : -49.931\n",
      "episode :  412, reward :  176.488, critic loss : 5.434, actor loss : -50.870\n",
      "episode :  413, reward :  280.539, critic loss : 5.226, actor loss : -52.566\n",
      "episode :  414, reward :  252.501, critic loss : 5.017, actor loss : -56.702\n",
      "episode :  415, reward :  291.087, critic loss : 5.329, actor loss : -59.980\n",
      "episode :  416, reward :  254.582, critic loss : 5.640, actor loss : -61.455\n",
      "episode :  417, reward :  24.603, critic loss : 6.656, actor loss : -62.014\n",
      "episode :  418, reward :  224.771, critic loss : 7.475, actor loss : -63.521\n",
      "episode :  419, reward :  249.126, critic loss : 9.152, actor loss : -66.247\n",
      "episode :  420, reward : -32.273, critic loss : 7.834, actor loss : -68.395\n",
      "episode :  421, reward :  35.043, critic loss : 7.580, actor loss : -68.519\n",
      "episode :  422, reward :  216.064, critic loss : 9.624, actor loss : -68.703\n",
      "episode :  423, reward :  259.015, critic loss : 11.280, actor loss : -69.417\n",
      "episode :  424, reward :  33.961, critic loss : 10.723, actor loss : -70.070\n",
      "episode :  425, reward :  278.584, critic loss : 14.549, actor loss : -70.392\n",
      "episode :  426, reward :  245.328, critic loss : 10.788, actor loss : -70.233\n",
      "episode :  427, reward : -10.578, critic loss : 9.586, actor loss : -70.353\n",
      "episode :  428, reward :  212.963, critic loss : 15.666, actor loss : -69.802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  429, reward :  257.808, critic loss : 11.321, actor loss : -70.328\n",
      "episode :  430, reward :  135.772, critic loss : 10.887, actor loss : -71.339\n",
      "episode :  431, reward :  276.336, critic loss : 11.969, actor loss : -72.461\n",
      "episode :  432, reward :  263.377, critic loss : 8.641, actor loss : -72.890\n",
      "episode :  433, reward :  192.295, critic loss : 10.740, actor loss : -73.279\n",
      "episode :  434, reward :  260.253, critic loss : 10.786, actor loss : -73.882\n",
      "episode :  435, reward :  156.458, critic loss : 9.240, actor loss : -74.629\n",
      "episode :  436, reward :  216.280, critic loss : 10.980, actor loss : -75.101\n",
      "episode :  437, reward :  190.922, critic loss : 11.148, actor loss : -74.946\n",
      "episode :  438, reward :  283.789, critic loss : 10.663, actor loss : -74.081\n",
      "episode :  439, reward :  270.239, critic loss : 15.178, actor loss : -74.511\n",
      "episode :  440, reward :  211.349, critic loss : 12.158, actor loss : -74.712\n",
      "episode :  441, reward :  275.276, critic loss : 8.423, actor loss : -74.961\n",
      "episode :  442, reward :  224.761, critic loss : 9.607, actor loss : -74.882\n",
      "episode :  443, reward :  150.817, critic loss : 12.535, actor loss : -74.514\n",
      "episode :  444, reward :  231.729, critic loss : 11.484, actor loss : -74.602\n",
      "episode :  445, reward :  4.006, critic loss : 7.013, actor loss : -74.116\n",
      "episode :  446, reward :  280.889, critic loss : 11.390, actor loss : -74.563\n",
      "episode :  447, reward :  243.779, critic loss : 16.893, actor loss : -75.245\n",
      "episode :  448, reward :  171.721, critic loss : 14.814, actor loss : -75.786\n",
      "episode :  449, reward :  161.574, critic loss : 13.605, actor loss : -76.637\n",
      "episode :  450, reward :  277.113, critic loss : 11.173, actor loss : -77.565\n",
      "episode :  451, reward :  263.601, critic loss : 7.197, actor loss : -77.168\n",
      "episode :  452, reward :  205.843, critic loss : 13.148, actor loss : -77.019\n",
      "episode :  453, reward :  225.522, critic loss : 7.412, actor loss : -77.342\n",
      "episode :  454, reward : -8.268, critic loss : 7.679, actor loss : -78.159\n",
      "episode :  455, reward :  302.897, critic loss : 16.618, actor loss : -78.644\n",
      "episode :  456, reward :  229.603, critic loss : 12.200, actor loss : -78.790\n",
      "episode :  457, reward :  268.963, critic loss : 9.209, actor loss : -79.581\n",
      "episode :  458, reward :  8.244, critic loss : 11.308, actor loss : -79.150\n",
      "episode :  459, reward :  248.299, critic loss : 9.170, actor loss : -79.345\n",
      "episode :  460, reward :  245.822, critic loss : 10.699, actor loss : -79.594\n",
      "episode :  461, reward :  227.724, critic loss : 11.676, actor loss : -79.611\n",
      "episode :  462, reward :  267.074, critic loss : 16.756, actor loss : -79.600\n",
      "episode :  463, reward :  247.648, critic loss : 13.290, actor loss : -79.724\n",
      "episode :  464, reward :  238.789, critic loss : 9.983, actor loss : -79.757\n",
      "episode :  465, reward :  257.171, critic loss : 14.535, actor loss : -79.825\n",
      "episode :  466, reward :  206.260, critic loss : 11.221, actor loss : -79.884\n",
      "episode :  467, reward :  35.367, critic loss : 15.296, actor loss : -80.727\n",
      "episode :  468, reward :  176.654, critic loss : 14.380, actor loss : -81.050\n",
      "episode :  469, reward :  227.611, critic loss : 20.283, actor loss : -81.124\n",
      "episode :  470, reward :  219.167, critic loss : 11.886, actor loss : -81.754\n",
      "episode :  471, reward :  257.264, critic loss : 9.954, actor loss : -81.482\n",
      "episode :  472, reward :  216.867, critic loss : 9.980, actor loss : -81.138\n",
      "episode :  473, reward :  212.429, critic loss : 11.396, actor loss : -80.645\n",
      "episode :  474, reward :  245.238, critic loss : 14.223, actor loss : -81.088\n",
      "episode :  475, reward :  255.265, critic loss : 11.839, actor loss : -81.767\n",
      "episode :  476, reward :  218.592, critic loss : 10.158, actor loss : -82.420\n",
      "episode :  477, reward : -133.464, critic loss : 9.566, actor loss : -82.703\n",
      "episode :  478, reward :  235.234, critic loss : 11.153, actor loss : -81.707\n",
      "episode :  479, reward :  258.564, critic loss : 12.021, actor loss : -81.824\n",
      "episode :  480, reward :  23.025, critic loss : 11.424, actor loss : -80.361\n",
      "episode :  481, reward :  269.706, critic loss : 10.586, actor loss : -79.291\n",
      "episode :  482, reward :  11.392, critic loss : 14.690, actor loss : -79.904\n",
      "episode :  483, reward :  260.753, critic loss : 22.746, actor loss : -80.598\n",
      "episode :  484, reward :  241.014, critic loss : 17.354, actor loss : -80.886\n",
      "episode :  485, reward :  251.979, critic loss : 13.657, actor loss : -81.394\n",
      "episode :  486, reward :  274.642, critic loss : 14.227, actor loss : -82.138\n",
      "episode :  487, reward :  229.349, critic loss : 10.629, actor loss : -82.443\n",
      "episode :  488, reward :  210.057, critic loss : 14.032, actor loss : -82.842\n",
      "episode :  489, reward :  218.297, critic loss : 10.202, actor loss : -83.240\n",
      "episode :  490, reward :  252.202, critic loss : 6.256, actor loss : -82.623\n",
      "episode :  491, reward :  254.028, critic loss : 7.125, actor loss : -83.326\n",
      "episode :  492, reward :  248.670, critic loss : 9.129, actor loss : -83.529\n",
      "episode :  493, reward :  265.700, critic loss : 10.897, actor loss : -83.945\n",
      "episode :  494, reward :  235.380, critic loss : 8.995, actor loss : -83.724\n",
      "episode :  495, reward :  273.259, critic loss : 7.318, actor loss : -84.415\n",
      "episode :  496, reward :  195.125, critic loss : 6.757, actor loss : -84.422\n",
      "episode :  497, reward :  233.848, critic loss : 8.156, actor loss : -84.530\n",
      "episode :  498, reward : -44.746, critic loss : 8.629, actor loss : -84.125\n",
      "episode :  499, reward :  241.901, critic loss : 6.314, actor loss : -83.398\n"
     ]
    }
   ],
   "source": [
    "# initialize environment / action\n",
    "env_name = \"LunarLanderContinuous-v2\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "action_low = env.action_space.low\n",
    "action_high = env.action_space.high\n",
    "\n",
    "################################################################\n",
    "# Task 5: Initialize the DDPGAgent on the training loop\n",
    "agent = DDPGAgent(state_dim, action_dim, action_low, action_high)\n",
    "################################################################\n",
    "\n",
    "MAX_EPISODES = 500\n",
    "\n",
    "# Performance metric\n",
    "rewards = []\n",
    "critic_loss_traj = []\n",
    "actor_loss_traj = []\n",
    "max_reward = 0.0\n",
    "\n",
    "for e in range(MAX_EPISODES):\n",
    "\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    step = 0\n",
    "    critic_loss_epi = []\n",
    "    actor_loss_epi = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = agent.get_action(state, step)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        transition = [state, action, reward, next_state, done]\n",
    "        \n",
    "        agent.push(transition) # Saves transition on every step\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "  \n",
    "        critic_loss, actor_loss = agent.fit()\n",
    "\n",
    "        critic_loss_epi.append(critic_loss)\n",
    "        actor_loss_epi.append(actor_loss)\n",
    "            \n",
    "        if done:\n",
    "            rewards.append(episode_reward)\n",
    "            critic_loss_traj += critic_loss_epi\n",
    "            actor_loss_traj += actor_loss_epi\n",
    "            \n",
    "            critic_mean = sum(critic_loss_epi) / len(critic_loss_epi)\n",
    "            actor_mean = sum(actor_loss_epi) / len(actor_loss_epi)\n",
    "            \n",
    "            print(\"episode : %4d, reward : % 4.3f, critic loss : %4.3f, actor loss : %4.3f\" % (e, rewards[-1], critic_mean, actor_mean))\n",
    "            break\n",
    "\n",
    "        step += 1\n",
    "    \n",
    "    avg5 = np.mean(rewards[-5:])\n",
    "    if avg5 > max_reward:\n",
    "        torch.save(agent.actor.state_dict(), \"actor.pth\")\n",
    "        torch.save(agent.critic.state_dict(), \"critic.pth\")\n",
    "        max_reward = avg5\n",
    "\n",
    "env.close()\n",
    "\n",
    "with open('rewards.pkl', 'wb') as f:\n",
    "    pickle.dump(rewards, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('actor_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(critic_loss_traj, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('critic_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(actor_loss_traj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('rewards.pkl', 'rb') as f:\n",
    "#     rewards = pickle.load(f)\n",
    "    \n",
    "# with open('critic_loss.pkl', 'rb') as f:\n",
    "#     critic_loss_traj = pickle.load(f)\n",
    "\n",
    "# with open('actor_loss.pkl', 'rb') as f:\n",
    "#     actor_loss_traj = pickle.load(f)\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.title('reward')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(critic_loss_traj)\n",
    "plt.title('critic loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(actor_loss_traj)\n",
    "plt.title('actor loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward :  256.484\n",
      "reward :  246.416\n",
      "reward :  116.758\n",
      "reward :  285.494\n",
      "reward :  249.361\n",
      "reward :  238.628\n",
      "reward :  207.064\n",
      "reward :  228.758\n",
      "reward :  251.011\n",
      "reward :  91.926\n",
      "Avg. Rewards: 217.1901281618704\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "env_name = \"LunarLanderContinuous-v2\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "action_low = env.action_space.low\n",
    "action_high = env.action_space.high\n",
    "\n",
    "agent = DDPGAgent(state_dim, action_dim, action_low, action_high)\n",
    "agent.actor.load_state_dict(torch.load(\"actor.pth\"))\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for e in range(10):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action = agent.get_action(state)\n",
    "#         env.render()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"reward : % 4.3f\" % (episode_reward))\n",
    "            rewards.append(episode_reward)\n",
    "            break\n",
    "\n",
    "        step += 1\n",
    "env.close()\n",
    "print(\"Avg. Rewards:\", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
